<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="website of DAniel F-S">
  <meta name="author" content="Daniel Fs">
  <title>Daniel Flam-Shepherd</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

</head>
<body>
  <div class="container">

    <div class="row" style="padding:10px">

      <div class="col-xs-12 col-sm-9 col-md-9">

        <div class="row">
          <img src="img/dan.png" class="pull-left" style="margin:20px
          20px 20px 0; height:325px; width:225px"/>
          <h1>Daniel Flam-Shepherd
          <!--a href="https://twitter.com/danielflamshepherd" class="icon">
            <i class="fa fa-twitter fa-lg"></i-->
          </a>
          <!--a href="https://github.com/flame02" class="icon">
            <i class="fa fa-github fa-lg"></i>
          </a--></h1>
          <p class="row">
            Hello, I just recently finished a Msc in Statistics at The University of Toronto,
            where I worked with
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>.
            Before that I was Graduate student in the Atmospheric Physics group under the supervision of
            <a href="http://www.atmosp.physics.utoronto.ca/~jones/">Dylan Jones</a>.
            I am widely interested in statistics and machine learning.
            Some current things i'm thinking about include:
            <br> 	&nbsp; &#9679; principaled priors for bayesian neural networks </br>
            	    &nbsp; &#9679; limits of mean field variational inference </br>
                  &nbsp; &#9679; hypernetworks and their bayesian variants </br>
                  &nbsp; &#9679; bayesian optimization for the large scale exploration of chemical space </br>
                  &nbsp; &#9679; generative models of graphs for automatic chemical design  </br>
                  &nbsp; &#9679; Bayesian approaches to fair machine learning  </br>
            If you're interested in any of these and want to work together please send me an email at
           danielfs@utstat.toronto.edu. You can also check out my <a href="resume.pdf">resume</a> if you want to. </br>
            <br>

          </p>
        </div>

        <hr>

        <!--div class="row">
          <p>
            <strong> Durring my Masters I took : </strong><br>
            STA2112/2212 : Mathematical Statistics I & II<br>
            STA2102/2202 : Applied Statististics I & II <br>
            STA4001/4002 : Research on Bayesian Neural Networks </a><br>
            STA4527 : Learning Discrete Latent structure <br>
            CSC2506 : Probabilistic Reasoning and Uncertainty <br>
          <p>
            I also TAed STA220, STA247, STA255, STA257 and STA302.
        </div>
        <hr>

        <!-- Projects Section -->
        <section id="pubs" class="pubs-section">
            <div class="container">
                <div class="row">
                    <h3> Preprints, Publications and Projects </h3>
                </div>
    <br>



    <div class="row">
        <div class="col-xs-2">
            <img src="img/cwbnn.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="158.pdf">Characterizing and warping the function space of Bayesian neural networks</a> </strong></br>
            In this work we develop a simple method to construct priors for Bayesian neural networks
            that incorporates meaningful prior information about functions. We fit neural nets to samples of functions
            using a hypernetwork, in order to use the empirical moments of the learned weights for our prior parameters.
            This method allows us to characterize the relationship between weight space and function space.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="http://jamesr.info/">James Requeima</a>,
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
            <br>
            <em>NIPS Workshop on Bayesian Deep Learning</em>, 2018 <br>
            <!--div class="btn-group-xs">
              <a href="2546Project.pdf"
              class="btn btn-default">Paper</a>
              <a href="https://github.com/flame02/csc2546project"
              class="btn btn-default">Code</a>
            </div-->
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/sbnp.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="49.pdf"> Stick breaking neural latent variable models </a></strong></br>

            Neural processes define a class of neural latent variable models.
            We extend this class to an infinite dimensional space by imposing a
            stick-breaking prior on the latent space. Using Stochastic Gradient
            Variational Bayes, we perform posterior inference for the weights of
            the stick-breaking process and develop the stick-breaking neural process (SB-NP).
            SB-NPs are able to learn the dimensionality of the latent space and
            have improved posterior uncertainty.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="http://www.utstat.toronto.edu/~ygao/">Yuxiang Gao</a>,
            <a href="https://www.linkedin.com/in/zhaoyu-guo-85866b72/">Zhaoyu Guo</a>
            <br>
            <em>NIPS Workshop on All of Bayesian Nonparametrics</em>, 2018 <br>
            <!--div class="btn-group-xs">
              <a href="2546Project.pdf"
              class="btn btn-default">Paper</a>
              <a href="https://github.com/flame02/csc2546project"
              class="btn btn-default">Code</a>
            </div-->
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/mgppbnn.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="65.pdf"> Mapping Gaussian process priors to Bayesian Neural Networks</a> </strong></br>
            What defines a reasonable prior to use in Bayesian models?
            Often, normal distributions are placed over the weights for convenience and
            are interpreted as a bias toward less complex functions via smaller weights.
            Gaussian processes, on the other hand, have a elegant mechanism for incorporating
            prior beliefs about the underlying function - specifying the mean and covariance functions.
            In this work, we present an approach to specify a more principled prior for
            Bayesian Neural Networks that can leverage the well studied kernel design techniques
            from Gaussian process regression. <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="http://jamesr.info/">James Requeima</a>,
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
            <br>
            <em>NIPS Workshop on Bayesian Deep Learning</em>, 2017 <br>
            <!--div class="btn-group-xs">
              <a href="2546Project.pdf"
              class="btn btn-default">Paper</a>
              <a href="https://github.com/flame02/csc2546project"
              class="btn btn-default">Code</a>
            </div-->
            <br>
        </div>
    </div>


                <div class="row">
                    <div class="col-xs-2">
                        <img src="img/fba.png" class="img-responsive"  alt="Surface plot depicting problem"/>
                    </div>
                    <div class="col-xs-8 text-left">
                        <strong><a href="2546Project.pdf"> Generalized Feedback Alignment</a> </strong></br>
                        The back-propagation algorithm is one of the main tools for credit assignment
                        in neural networks where the loss gradient is computed to back-propagate error
                        from the output layer to the hidden layers. A method called feedback alignment
                        performs almost as well and is more biologically plausible since it avoids using
                        the weights from the forward pass in the the backwards pass by replacing them
                        with random feedback weights. <!--Direct feedback alignment is even more biologically
                        plausible by decoupling the error signal for each layer, such that the error is
                        not back-propagated through the layers but rather proceeds directly to each hidden
                        layer.--> In this work, a general feedback alignment strategy for training
                        neural networks is proposed and experimented with in an supervised and unsupervised setting
                        <br>
                        <!--div class="btn-group-xs">
                          <a href="2546Project.pdf"
                          class="btn btn-default">Paper</a>
                          <a href="https://github.com/flame02/csc2546project"
                          class="btn btn-default">Code</a>
                        </div-->
                        <br>
                    </div>
                </div>

                <div class="row">
                    <div class="col-xs-2">
                        <img src="img/bnnpbl.png" class="img-responsive"  alt="Surface plot depicting problem"/>
                    </div>
                    <div class="col-xs-8 text-left">
                        <strong> Predicting planetary boundary layer height with Bayesian Neural Networks </strong></br>
                        During my M SC. of Physics
                        I conducted research in Atmospheric Physics. Specifically, I used bayesian
                        neural networks to learn a functional relationship between the planetary
                        boundary layer depth and physical quantities commonly measured at weather
                        stations such as temperature, wind speeds, surface pressure speciﬁc humidity
                        and solar intensity.
                        <br><br>
                        <!--div class="btn-group-xs">
                          <a href="msc.pdf"
                          class="btn btn-default">MSc Report Paper</a>
                          <a href="https://github.com/flame02/MSc-Project"
                          class="btn btn-default">Code</a>
                        </div--!>
                        <br>
                    </div>
                </div>


      <hr>

      </div>

    </div>



    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
