<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="website of DantheMan FS">
  <meta name="author" content="Daniel Fs">
  <title>Daniel Flam-Shepherd</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

</head>
<body>
  <div class="container">
      <div class="col-xs-20 col-sm-11 col-md-10">
        <!--div class="row"-->
          <img src="img/danjet.jpg" class="pull-right" style="margin:20px
          20px 20px 20px; height:280px; width:500"/-->
          <h1>Daniel Flam-Shepherd</h1>
          <!--a href="https://twitter.com/danielflamshepherd" class="icon">
            <i class="fa fa-twitter fa-lg"></i-->

          <!--a href="https://github.com/flame02" class="icon">
            <i class="fa fa-github fa-lg"></i>
          </a-->
          <p class="row">
            I'm a machine learning Ph.D. student at the University of Toronto in the
            <a href="https://web.cs.toronto.edu/"> Department of Computer Science </a>
            and the <a href="https://vectorinstitute.ai/research/"> Vector Institute</a>.
            My supervisor is <a href="https://matter.toronto.edu/"> Alan Aspuru-Guzik</a>.
            My research interests include:
            <br>  &nbsp; &#9679; Deep generative models for scientific discovery </br>
                  &nbsp; &#9679; 3D molecular design using reinforcement learning </br>
            Previously I worked on priors for Bayesian neural networks with
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>.

            <br>

          </p>
        </div>

        <hr>
        <br>

        <!-- Projects Section -->
      <!--section id="pubs" class="pubs-section">
          <div class="container">
              <div class="row">
                  <h3> Preprints, Publications and Projects </h3>
              </div-->
    <br>

    <div class="row">
        <div class="col-xs-8 text-left">
            <strong>  <h2> Papers, preprints and projects </h2></strong></br>

            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/rnn.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="https://arxiv.org/pdf/2112.03041"> Keeping it Simple: Language Models can learn Complex Molecular Distributions </a> </strong></br>
            Deep generative models of molecules have grown immensely in popularity, trained on relevant datasets, these models are used to search through chemical space. The downstream utility of generative models for the inverse design of novel functional compounds depends on their ability to learn a training distribution of molecules. The most simple example is a language model that takes the form of a recurrent neural network and generates molecules using a string representation. More sophisticated are graph generative models, which sequentially construct molecular graphs and typically achieve state of the art results. However, recent work has shown that language models are more capable than once thought, particularly in the low data regime. In this work, we investigate the capacity of simple language models to learn distributions of molecules. For this purpose, we introduce several challenging generative modeling tasks by compiling especially complex distributions of molecules. On each task, we evaluate the ability of language models as compared with two widely used graph generative models. The results demonstrate that language models are powerful generative models, capable of adeptly learning complex molecular distributions -- and yield better performance than the graph models. Language models can accurately generate: distributions of the highest scoring penalized LogP molecules in ZINC15, multi-modal molecular distributions as well as the largest molecules in PubChem.            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="https://ca.linkedin.com/in/kevin-qy-zhu">Kevin Zhu</a>,
            <a href="https://www.matter.toronto.edu/basic-content-page/about-alan">Alan Aspuru-Guzik</a>
            <br>
            <em> preprint </em>, 2021 <br>
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/qo.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="https://arxiv.org/pdf/2109.02490.pdf"> 
            Learning Interpretable Representations of Entanglement in Quantum Optics Experiments <br> using Deep Generative Models </a> </strong></br>
            Quantum physics experiments produce interesting phenomena such as interference or entanglement, 
            which is a core property of numerous future quantum technologies. 
            The complex relationship between a quantum experiment's structure and its entanglement properties is essential to fundamental research in quantum optics but is difficult to intuitively understand.
            We present the first deep generative model of quantum optics experiments where a variational autoencoder (QOVAE) is trained on a dataset of experimental setups. 
            In a series of computational experiments, we investigate the learned representation of the QOVAE and its internal understanding of the quantum optics world. 
            We demonstrate that the QOVAE learns an intrepretable representation of quantum optics experiments and the relationship between experiment structure and entanglement.
            We show the QOVAE is able to generate novel experiments for highly entangled quantum states with specific distributions that match its training data. 
            Importantly, we are able to fully interpret how the QOVAE structures its latent space, finding curious patterns that we can entirely explain in terms of quantum physics. 
            The results demonstrate how we can successfully use and understand the internal representations of deep generative models in a complex scientific domain. 
            The QOVAE and the insights from our investigations can be immediately applied to other physical systems throughout fundamental scientific research.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="https://scholar.google.com/citations?user=QxD7RqQAAAAJ&hl=en">Tony Wu</a>,
            <a href="https://scholar.google.com/citations?user=iqzPpJcAAAAJ">Xumei Gu</a>,
            <a href="https://albacl.github.io/">Alba Cervera Lierta</a>,
            <a href="https://mariokrenn.wordpress.com/">Mario Krenn</a>,
            <a href="https://www.matter.toronto.edu/basic-content-page/about-alan">Alan Aspuru-Guzik</a>
            <br>
            <em> preprint </em>, 2021 <br>
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/gvae.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="https://arxiv.org/pdf/2002.07087.pdf"> Graph Deconvolutional Generation </a> </strong></br>
            Graph generation is an extremely important task, as graphs are found throughout different areas of science and engineering.
            In this work, we focus on the modern equivalent of the Erdos-Renyi random graph model: the graph variational autoencoder (GVAE).
            This model assumes edges and nodes are independent in order to generate entire graphs at a time using a multi-layer perceptron decoder.
            As a result of these assumptions, GVAE has difficulty matching the training distribution and relies on an expensive graph matching procedure.
            We improve this class of models by building a message passing neural network into GVAE's encoder and decoder.
            We demonstrate our model on the specific task of generating small organic molecules.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="https://scholar.google.com/citations?user=QxD7RqQAAAAJ&hl=en">Tony Wu</a>,
            <a href="https://www.matter.toronto.edu/basic-content-page/about-alan">Alan Aspuru-Guzik</a>
            <br>
            <em> preprint </em>, 2019 <br>
            <br>
        </div>
    </div>


    <div class="row">
        <div class="col-xs-2">
            <img src="img/path.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="https://arxiv.org/abs/2002.10413"> Neural Message Passing on Higher Order Paths </a> </strong></br>
            Graph neural network (GNNs) have achieved impressive results in predicting molecular properties,
            but do not directly account for local and hidden structures such as functional groups and molecular geometry.
            At every layer, GNNs aggregate only over first order neighbours, ignoring important
            information contained in subsequent neighbours as well as the relationships between those higher order connections.
            In this work, we generalize graph neural nets to pass messages and aggregate across
            higher order paths. This allows information to propagate over various levels and substructures of the graph.
            We demonstrate our model on a few tasks in molecular property prediction.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="https://scholar.google.com/citations?user=QxD7RqQAAAAJ&hl=en">Tony Wu</a>
            <a href="https://aimat.iti.kit.edu/">Pascal Friederich</a>
            <a href="https://www.matter.toronto.edu/basic-content-page/about-alan">Alan Aspuru-Guzik</a>
            <br>
            <em> preprint </em>, 2019 <br>
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/cwbnn.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="158.pdf">Characterizing and warping the function space of Bayesian neural networks</a> </strong></br>
            In this work we develop a simple method to construct priors for Bayesian neural networks
            that incorporates meaningful prior information about functions. We fit neural nets to samples of functions
            using a hypernetwork, in order to use the empirical moments of the learned weights for our prior parameters.
            This method allows us to characterize the relationship between weight space and function space.
            <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="http://jamesr.info/">James Requeima</a>,
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
            <br>
            <em>NIPS Workshop on Bayesian Deep Learning</em>, 2018 <br>
            <!--div class="btn-group-xs">
              <a href="2546Project.pdf"
              class="btn btn-default">Paper</a>
              <a href="https://github.com/flame02/csc2546project"
              class="btn btn-default">Code</a>
            </div-->
            <br>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-2">
            <img src="img/mgppbnn.png" class="img-responsive"  alt="Surface plot depicting problem"/>
        </div>
        <div class="col-xs-8 text-left">
            <strong><a href="65.pdf"> Mapping Gaussian process priors to Bayesian Neural Networks</a> </strong></br>
            What defines a reasonable prior to use in Bayesian models?
            Often, normal distributions are placed over the weights for convenience and
            are interpreted as a bias toward less complex functions via smaller weights.
            Gaussian processes, on the other hand, have a elegant mechanism for incorporating
            prior beliefs about the underlying function - specifying the mean and covariance functions.
            In this work, we present an approach to specify a more principled prior for
            Bayesian Neural Networks that can leverage the well studied kernel design techniques
            from Gaussian process regression. <br>
            <a href="https://danielflamshep.github.io./">Daniel Flam-Shepherd</a>,
            <a href="http://jamesr.info/">James Requeima</a>,
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
            <br>
            <em>NIPS Workshop on Bayesian Deep Learning</em>, 2017 <br>
            <!--div class="btn-group-xs">
              <a href="2546Project.pdf"
              class="btn btn-default">Paper</a>
              <a href="https://github.com/flame02/csc2546project"
              class="btn btn-default">Code</a>
            </div-->
            <br>
        </div>                        <br>
    </div>

      <hr>

      </div>

    </div>



    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
